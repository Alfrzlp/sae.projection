---
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# sae.projection

<!-- badges: start -->

[![CRAN status](https://www.r-pkg.org/badges/version/sae.projection)](https://CRAN.R-project.org/package=sae.projection) [![R-CMD-check](https://github.com/Alfrzlp/sae.projection/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/Alfrzlp/sae.projection/actions/workflows/R-CMD-check.yaml)

<!-- badges: end -->

## Author

Azka Ubaidillah, Ridson Al Farizal P, Silvi Ajeng Larasati, Amelia Rahayu

## Maintainer

Ridson Al Farizal P [ridsonap\@bps.go.id](mailto:ridsonap@bps.go.id)

## Description

The **sae.projection** package provides a robust tool for *small area estimation using a projection-based approach*. This method is particularly beneficial in scenarios involving two surveys, the first survey collects data solely on auxiliary variables, while the second, typically smaller survey, collects both the variables of interest and the auxiliary variables. The package constructs a working model to predict the variables of interest for each sample in the first survey. These predictions are then used to estimate relevant indicators for the desired domains. This condition overcomes the problem of estimation in a small area when only using the second survey data.

## Installation

You can install the development version of sae.projection from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("Alfrzlp/sae.projection")
```

## Example

This is a basic example which shows you how to solve a common problem:

```{r example}
library(sae.projection)
library(dplyr)
```

## Regression

### Data

```{r}
df_svy22_income <- df_svy22 %>% filter(!is.na(income))
df_svy23_income <- df_svy23 %>% filter(!is.na(income))
```

### Linear Regression Model

```{r example1}
lm_proj <- ma_projection(
  income ~ age + sex + edu + disability,
  cluster_ids = "PSU", weight = "WEIGHT", strata = "STRATA",
  domain = c("PROV", "REGENCY"),
  working_model = linear_reg(),
  data_model = df_svy22_income,
  data_proj = df_svy23_income,
  nest = TRUE
)

lm_proj$projection
```

## Classification

### Data

```{r}
df_svy22_neet <- df_svy22 %>%
  filter(between(age, 15, 24))
df_svy23_neet <- df_svy23 %>%
  filter(between(age, 15, 24))
```

### Logistic Regression

```{r example3}
lr_proj <- ma_projection(
  formula = neet ~ sex + edu + disability,
  cluster_ids = "PSU",
  weight = "WEIGHT",
  strata = "STRATA",
  domain = c("PROV", "REGENCY"),
  working_model = logistic_reg(),
  data_model = df_svy22_neet,
  data_proj = df_svy23_neet,
  nest = TRUE
)

lr_proj$projection
```

### LightGBM with Hyperparameter Tunning

```{r}
library(bonsai)
show_engines("boost_tree")
lgbm_model <- boost_tree(
  mtry = tune(), trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),
  engine = "lightgbm"
)
```

```{r example4, eval=FALSE}
lgbm_proj <- ma_projection(
  formula = neet ~ sex + edu + disability,
  cluster_ids = "PSU",
  weight = "WEIGHT",
  strata = "STRATA",
  domain = c("PROV", "REGENCY"),
  working_model = lgbm_model,
  data_model = df_svy22_neet,
  data_proj = df_svy23_neet,
  cv_folds = 3,
  grid = 20,
  nest = TRUE
)

lgbm_proj$projection
```

### Projection Estimator with Random Forest Algorithm

```{r}
data(df_svy_A)
data(df_svy_B)
x_predictors <- names(df_svy_A)[5:19]

# Projection RF with feature selection and bias correction
result <- projection_randomforest(
  data_model = df_svy_A,
  target_column = "Y",
  predictor_cols = x_predictors,
  data_proj = df_svy_B,
  domain1 = "province",
  domain2 = "regency",
  psu = "num",
  ssu = NULL,
  strata = NULL,
  weights = "weight",
  feature_selection = TRUE,
  bias_correction = TRUE
)
print(result)
```

### Projection Estimator with XGBoost Algorithm

```{r}
data(df_svy_A)
data(df_svy_B)

# Remove unused identifier column
df_svy_A <- df_svy_A |> dplyr::select(-id_ind)
df_svy_B <- df_svy_B |> dplyr::select(-id_ind)

# Projection XGBoost with feature selection and bias correction
result <- projection_xgboost(
  target_col = "Y",
  data_model = df_svy_A,
  data_proj = df_svy_B,
  id = "num",
  STRATA = NULL,
  domain1 = "province",
  domain2 = "regency",
  weight = "weight",
  nfold = 10,
  test_size = 0.2,
  task_type = "classification",
  feature_selection = TRUE,
  corrected_bias = TRUE
)
print(result)
```
